###############################################################################
#        Copyright Zalsoft Inc                        2006                    #
###############################################################################
#this is a comment
# parameter   = value #this is a line followd by a comment

###############################################################################
name    =   a given name                  # a name at your will this agent will crawl. (max 32 characters)
email   =   youremail.com                 # MANDATORY VALID EMAIL SO SOME CAN CAMPLAIN ABOU YOUR CRAWLER
   

###############################################################################
livefile           =  /home/htdocs/logs/buzznet.txt   
reportfile          = /home/htdocs/logs/report0     # all scanned site saved files are logged here                                            
queue backup        = /home/htdocs/logs/queue0      # URL QUEUE BACKUP DEFAULT 'QUEUE.BK' (tune up parameter)                                 
                                                

                                           # to this port
print out           = 1111100              # feed to the client/console connected 
# order of bits: info,warn,error,header,rec,send,inq,???
#print out          = url,content           # feed to the client/console connected 
                                            
                                            
###############################################################################                                            
parameters          = 1 # TAKE PARAMETERS FOR URL LINKS TOO NAVIGATE 
delay               = 3000 # DELAY BETWEEN POOLLING URLS IN MILISECONDS. 

###############################################################################
pagespersite   = 640                # maximum pages per site   
external sites = 1                  # GOES IN EXTERNAL SITES (1)
depth search   = 5                  # HOW DEEP TO GO 
ingore robots  = 1                  # dont ignore the robots couse you end up in IP BAN

###############################################################################

queue length   = 4096                # range(128 - 4096)URL WAITING QUEUE (tune up parameter)

###############################################################################
connections   = 12                   # SIMULTANEOUS OPENED CONNECTIONS max 63 (tune up parameter)
timeout       = 3000                 # TIMEOUT FOR CONNECTION (1000-30000) (conection timeout)
retryes       = 2                    # RETRY A TIMEOUT CONNECTION (max 5)  (conection retry) 

###############################################################################
writelocation   = /_WEBS/           # WHERE TO SAVE SCANNED WEBS. def: ./_WEB/
bzip            = 0                 # to compress the file when is storing it                   
################# how to save the crawl the web sites ##########################
writerule       = deep 
#writerule      = asis               # 
#writerule      = by ip,asis         # under IP/www.whaterver.com/<target>/<file>_<params> (? replaced by #)
#writerule      = by site            # under www.whaterver.com/<page>_<params>
#writerule      = by ip              # under xxx.xxx.xxx.xxx//<www>_<page>_<params>
#writerule      = by ip,by site      # under xxx.xxx.xxx.xxx/www.whaterver.com/<page>_<params>

###############################################################################

deny      = $google,$msn,$altavista,$wikipedia,^pdf,^jpg,^gif,^zip,^rar    # $anywhere in url  
allow     = ^htm,^html,^shtm,^shtml,^php,             # ^at the end of url (extensions of documents)
allow     = ^php3,^php4,^asp,^aspx,^jsp,^psp,^pl
allow     = ^cgi,^stx,^cfm,^cfml,^cms,^/

###############################################################################
3 where to connect woth a web browser to see it's statuses
httpserver = 8000
